{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7700770,"sourceType":"datasetVersion","datasetId":4495179}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport transformers\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import DistilBertModel, DistilBertTokenizer","metadata":{"id":"X9FmZ7IFGXbh","execution":{"iopub.status.busy":"2024-02-25T23:41:18.377369Z","iopub.execute_input":"2024-02-25T23:41:18.377807Z","iopub.status.idle":"2024-02-25T23:41:27.305047Z","shell.execute_reply.started":"2024-02-25T23:41:18.377748Z","shell.execute_reply":"2024-02-25T23:41:27.304293Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from torch import cuda\ndevice = 'cuda' if cuda.is_available() else 'cpu'\ndevice","metadata":{"id":"12xsM-Y09WeC","execution":{"iopub.status.busy":"2024-02-25T23:41:27.306719Z","iopub.execute_input":"2024-02-25T23:41:27.307317Z","iopub.status.idle":"2024-02-25T23:41:27.367019Z","shell.execute_reply.started":"2024-02-25T23:41:27.307283Z","shell.execute_reply":"2024-02-25T23:41:27.365885Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/hackforimpact2024/final_expanded_dataset.csv')\n\n# Define the categories and their encoding\ntags = ['Funding', 'Operations', 'Misc', 'Food', 'Equipment', 'Programming', 'Travel']\nmap = {tag: i for i, tag in enumerate(tags)}\n\n# Encode the categories with a safety check\ndef encode_tags(x):\n    # Return the encoded value if x exists in encode_dict, otherwise return a placeholder or error code\n    return map.get(x, -1)  # -1 or any other value you choose as a placeholder for unknown categories\n\ndf['Tags_Encoded'] = df['Tags'].apply(lambda x: encode_tags(x))\n\ndf","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"EE3ozm52Gh4a","outputId":"f06ef9de-2b46-489d-aa2b-b39dce46bd3c","execution":{"iopub.status.busy":"2024-02-25T23:41:30.237899Z","iopub.execute_input":"2024-02-25T23:41:30.238748Z","iopub.status.idle":"2024-02-25T23:41:30.318232Z","shell.execute_reply.started":"2024-02-25T23:41:30.238713Z","shell.execute_reply":"2024-02-25T23:41:30.317312Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                       Memo        Tags  Tags_Encoded\n0          TRANSFER TO STATE HIGH HACK CLUB     Funding             0\n1                    NAME-CHEAP.COM* 8SG11P  Operations             1\n2                TRANSFER FROM HACK CLUB HQ     Funding             0\n3                            NAME-CHEAP.COM  Operations             1\n4      HACK CLUB BANK FEE (MISTAKE BY BANK)        Misc             2\n...                                     ...         ...           ...\n24995                Office furniture Chair   Equipment             4\n24996     Laptop purchase - Lenovo ThinkPad   Equipment             4\n24997                   Tech supplies order   Equipment             4\n24998                   Tech supplies order   Equipment             4\n24999                   Tech supplies order   Equipment             4\n\n[25000 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Memo</th>\n      <th>Tags</th>\n      <th>Tags_Encoded</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TRANSFER TO STATE HIGH HACK CLUB</td>\n      <td>Funding</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NAME-CHEAP.COM* 8SG11P</td>\n      <td>Operations</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TRANSFER FROM HACK CLUB HQ</td>\n      <td>Funding</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NAME-CHEAP.COM</td>\n      <td>Operations</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HACK CLUB BANK FEE (MISTAKE BY BANK)</td>\n      <td>Misc</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>24995</th>\n      <td>Office furniture Chair</td>\n      <td>Equipment</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>24996</th>\n      <td>Laptop purchase - Lenovo ThinkPad</td>\n      <td>Equipment</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>24997</th>\n      <td>Tech supplies order</td>\n      <td>Equipment</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>24998</th>\n      <td>Tech supplies order</td>\n      <td>Equipment</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>24999</th>\n      <td>Tech supplies order</td>\n      <td>Equipment</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n<p>25000 rows Ã— 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df['Tags'].value_counts()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"14rtqW8jhOpN","outputId":"2a25c250-1081-45ea-f3c8-cfef1b880bb4","execution":{"iopub.status.busy":"2024-02-25T23:41:31.056768Z","iopub.execute_input":"2024-02-25T23:41:31.057372Z","iopub.status.idle":"2024-02-25T23:41:31.075866Z","shell.execute_reply.started":"2024-02-25T23:41:31.057340Z","shell.execute_reply":"2024-02-25T23:41:31.074847Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"Tags\nProgramming    3820\nEquipment      3808\nTravel         3776\nOperations     3711\nFood           3651\nMisc           3128\nFunding        3106\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"MAX_LEN = 512\nTRAIN_BATCH_SIZE = 64\nVALID_BATCH_SIZE = 32\nEPOCHS = 3\nLEARNING_RATE = 1e-05\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')","metadata":{"id":"VWc9qUTS8lTk","execution":{"iopub.status.busy":"2024-02-26T00:01:59.810363Z","iopub.execute_input":"2024-02-26T00:01:59.811107Z","iopub.status.idle":"2024-02-26T00:02:00.015808Z","shell.execute_reply.started":"2024-02-26T00:01:59.811076Z","shell.execute_reply":"2024-02-26T00:02:00.015034Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class Triage(Dataset):\n    def __init__(self, dataframe, tokenizer, max_len):\n        self.len = len(dataframe)\n        self.data = dataframe\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __getitem__(self, index):\n        memo = str(self.data.Memo[index])\n        memo = \" \".join(memo.split())\n        inputs = self.tokenizer.encode_plus(\n            memo,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            pad_to_max_length=True,\n            return_token_type_ids=True,\n            truncation=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n\n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'targets': torch.tensor(self.data.Tags_Encoded[index], dtype=torch.long)\n        }\n\n    def __len__(self):\n        return self.len","metadata":{"id":"MFBMIX3cN1pG","execution":{"iopub.status.busy":"2024-02-25T23:41:32.716842Z","iopub.execute_input":"2024-02-25T23:41:32.717145Z","iopub.status.idle":"2024-02-25T23:41:32.725630Z","shell.execute_reply.started":"2024-02-25T23:41:32.717118Z","shell.execute_reply":"2024-02-25T23:41:32.724672Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_size = 0.7\ntrain_dataset=df.sample(frac=train_size,random_state=200)\ntest_dataset=df.drop(train_dataset.index).reset_index(drop=True)\ntrain_dataset = train_dataset.reset_index(drop=True)\n\n\nprint(\"FULL Dataset: {}\".format(df.shape))\nprint(\"TRAIN Dataset: {}\".format(train_dataset.shape))\nprint(\"TEST Dataset: {}\".format(test_dataset.shape))\n\ntraining_set = Triage(train_dataset, tokenizer, MAX_LEN)\ntesting_set = Triage(test_dataset, tokenizer, MAX_LEN)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_A26_OUQN4I3","outputId":"71117dd1-820e-4098-ea7d-6ca94c4a24db","execution":{"iopub.status.busy":"2024-02-25T23:41:32.804754Z","iopub.execute_input":"2024-02-25T23:41:32.805046Z","iopub.status.idle":"2024-02-25T23:41:32.820863Z","shell.execute_reply.started":"2024-02-25T23:41:32.805022Z","shell.execute_reply":"2024-02-25T23:41:32.819860Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"FULL Dataset: (25000, 3)\nTRAIN Dataset: (17500, 3)\nTEST Dataset: (7500, 3)\n","output_type":"stream"}]},{"cell_type":"code","source":"train_params = {'batch_size': TRAIN_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ntest_params = {'batch_size': VALID_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ntraining_loader = DataLoader(training_set, **train_params)\ntesting_loader = DataLoader(testing_set, **test_params)","metadata":{"id":"78Y9HrbdN7Xg","execution":{"iopub.status.busy":"2024-02-25T23:41:33.085225Z","iopub.execute_input":"2024-02-25T23:41:33.085560Z","iopub.status.idle":"2024-02-25T23:41:33.091241Z","shell.execute_reply.started":"2024-02-25T23:41:33.085533Z","shell.execute_reply":"2024-02-25T23:41:33.090040Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class DistillBERTClass(torch.nn.Module):\n    def __init__(self):\n        super(DistillBERTClass, self).__init__()\n        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n        self.pre_classifier = torch.nn.Linear(768, 768)\n        self.dropout = torch.nn.Dropout(0.3)\n        self.classifier = torch.nn.Linear(768, 7)\n\n    def forward(self, input_ids, attention_mask):\n        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n        hidden_state = output_1[0]\n        pooler = hidden_state[:, 0]\n        pooler = self.pre_classifier(pooler)\n        pooler = torch.nn.ReLU()(pooler)\n        pooler = self.dropout(pooler)\n        output = self.classifier(pooler)\n        return output","metadata":{"id":"-cc4non4N9Zx","execution":{"iopub.status.busy":"2024-02-25T23:41:33.348911Z","iopub.execute_input":"2024-02-25T23:41:33.349234Z","iopub.status.idle":"2024-02-25T23:41:33.356692Z","shell.execute_reply.started":"2024-02-25T23:41:33.349207Z","shell.execute_reply":"2024-02-25T23:41:33.355767Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"model = DistillBERTClass()\nmodel = torch.nn.DataParallel(model, device_ids = [0,1]).to(device)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MiwBoaeAOE9o","outputId":"d4f529dc-db2b-4cda-cfad-28ae8c3931ab","execution":{"iopub.status.busy":"2024-02-25T23:41:33.670118Z","iopub.execute_input":"2024-02-25T23:41:33.670484Z","iopub.status.idle":"2024-02-25T23:41:36.450648Z","shell.execute_reply.started":"2024-02-25T23:41:33.670454Z","shell.execute_reply":"2024-02-25T23:41:36.449776Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f8b87e0e2aa48798364ee72078d2cd7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fd43b44784d420f9593d96f4db39057"}},"metadata":{}}]},{"cell_type":"code","source":"loss_function = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)","metadata":{"id":"gOXJssQTOGJX","execution":{"iopub.status.busy":"2024-02-25T23:41:38.151053Z","iopub.execute_input":"2024-02-25T23:41:38.151417Z","iopub.status.idle":"2024-02-25T23:41:38.159415Z","shell.execute_reply.started":"2024-02-25T23:41:38.151379Z","shell.execute_reply":"2024-02-25T23:41:38.156395Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def calculate_accu(big_idx, targets):\n    n_correct = (big_idx==targets).sum().item()\n    return n_correct","metadata":{"id":"8_GS0Yz1OIUn","execution":{"iopub.status.busy":"2024-02-25T23:41:38.656330Z","iopub.execute_input":"2024-02-25T23:41:38.656689Z","iopub.status.idle":"2024-02-25T23:41:38.661980Z","shell.execute_reply.started":"2024-02-25T23:41:38.656659Z","shell.execute_reply":"2024-02-25T23:41:38.660695Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Defining the training function on the 80% of the dataset for tuning the distilbert model\n\ndef train(epoch):\n    tr_loss = 0\n    n_correct = 0\n    nb_tr_steps = 0\n    nb_tr_examples = 0\n    model.train()\n    for _,data in enumerate(training_loader, 0):\n        ids = data['ids'].to(device, dtype = torch.long)\n        mask = data['mask'].to(device, dtype = torch.long)\n        targets = data['targets'].to(device, dtype = torch.long)\n\n        outputs = model(ids, mask)\n        loss = loss_function(outputs, targets)\n        tr_loss += loss.item()\n        big_val, big_idx = torch.max(outputs.data, dim=1)\n        n_correct += calculate_accu(big_idx, targets)\n\n        nb_tr_steps += 1\n        nb_tr_examples+=targets.size(0)\n\n        if _%TRAIN_BATCH_SIZE==0:\n          loss_step = tr_loss/nb_tr_steps\n          accu_step = (n_correct*100)/nb_tr_examples\n          print(f\"Batch Training Loss: {loss_step}\")\n          print(f\"Batch Training Accuracy: {accu_step}\")\n\n        optimizer.zero_grad()\n        loss.backward()\n        # # When using GPU\n        optimizer.step()\n\n    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n    epoch_loss = tr_loss/nb_tr_steps\n    epoch_accu = (n_correct*100)/nb_tr_examples\n    print(f\"Training Loss Epoch: {epoch_loss}\")\n    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n\n    return","metadata":{"id":"h1B2unEAON4D","execution":{"iopub.status.busy":"2024-02-25T23:41:39.017592Z","iopub.execute_input":"2024-02-25T23:41:39.018409Z","iopub.status.idle":"2024-02-25T23:41:39.030982Z","shell.execute_reply.started":"2024-02-25T23:41:39.018371Z","shell.execute_reply":"2024-02-25T23:41:39.029838Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"for epoch in range(EPOCHS):\n    print(f'EPOCH {epoch}:\\n')\n    print('-'*128)\n    train(epoch)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":492},"id":"aX62yjVBOT74","outputId":"122e3282-be7b-4cc5-efbb-44d9d0085de4","execution":{"iopub.status.busy":"2024-02-25T23:41:39.517115Z","iopub.execute_input":"2024-02-25T23:41:39.517571Z","iopub.status.idle":"2024-02-26T00:00:55.689308Z","shell.execute_reply.started":"2024-02-25T23:41:39.517538Z","shell.execute_reply":"2024-02-26T00:00:55.688377Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"EPOCH 0:\n\n--------------------------------------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2619: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Batch Training Loss: 1.9759502410888672\nBatch Training Accuracy: 10.9375\nBatch Training Loss: 1.5817774864343497\nBatch Training Accuracy: 61.39423076923077\nBatch Training Loss: 1.0476430186698602\nBatch Training Accuracy: 78.35513565891473\nBatch Training Loss: 0.7659604588012003\nBatch Training Accuracy: 84.577396373057\nBatch Training Loss: 0.6084414187833261\nBatch Training Accuracy: 87.77358949416342\nThe Total Accuracy for Epoch 0: 88.35428571428571\nTraining Loss Epoch: 0.5779485600952902\nTraining Accuracy Epoch: 88.35428571428571\nEPOCH 1:\n\n--------------------------------------------------------------------------------------------------------------------------------\nBatch Training Loss: 0.16220742464065552\nBatch Training Accuracy: 96.875\nBatch Training Loss: 0.10188724582011884\nBatch Training Accuracy: 97.88461538461539\nBatch Training Loss: 0.09517648651502854\nBatch Training Accuracy: 97.90455426356588\nBatch Training Loss: 0.08916577992121173\nBatch Training Accuracy: 97.96794041450777\nBatch Training Loss: 0.08156892547840507\nBatch Training Accuracy: 98.17607003891051\nThe Total Accuracy for Epoch 1: 98.19428571428571\nTraining Loss Epoch: 0.08104334016378126\nTraining Accuracy Epoch: 98.19428571428571\nEPOCH 2:\n\n--------------------------------------------------------------------------------------------------------------------------------\nBatch Training Loss: 0.017663028091192245\nBatch Training Accuracy: 100.0\nBatch Training Loss: 0.04534359108656645\nBatch Training Accuracy: 98.9423076923077\nBatch Training Loss: 0.047055720532125284\nBatch Training Accuracy: 98.90988372093024\nBatch Training Loss: 0.04665359883651215\nBatch Training Accuracy: 98.91515544041451\nBatch Training Loss: 0.04822397378458123\nBatch Training Accuracy: 98.83268482490273\nThe Total Accuracy for Epoch 2: 98.81714285714285\nTraining Loss Epoch: 0.048328122307621214\nTraining Accuracy Epoch: 98.81714285714285\n","output_type":"stream"}]},{"cell_type":"code","source":"def valid(model, testing_loader):\n    model.eval()\n    n_correct = 0; n_wrong = 0; total = 0; tr_loss = 0; nb_tr_steps = 0; nb_tr_examples = 0;\n    with torch.no_grad():\n        for _, data in enumerate(testing_loader, 0):\n            ids = data['ids'].to(device, dtype = torch.long)\n            mask = data['mask'].to(device, dtype = torch.long)\n            targets = data['targets'].to(device, dtype = torch.long)\n            outputs = model(ids, mask).squeeze()\n            loss = loss_function(outputs, targets)\n            tr_loss += loss.item()\n            big_val, big_idx = torch.max(outputs.data, dim=1)\n            n_correct += calculate_accu(big_idx, targets)\n\n            nb_tr_steps += 1\n            nb_tr_examples+=targets.size(0)\n\n            if _%VALID_BATCH_SIZE==0:\n                loss_step = tr_loss/nb_tr_steps\n                accu_step = (n_correct*100)/nb_tr_examples\n                print(f\"Batch Validation Loss: {loss_step}\")\n                print(f\"Batch Validation Accuracy: {accu_step}\")\n    epoch_loss = tr_loss/nb_tr_steps\n    epoch_accu = (n_correct*100)/nb_tr_examples\n    print(f\"Validation Loss Epoch: {epoch_loss}\")\n    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n\n    return epoch_accu\n","metadata":{"id":"H7jxjTv2OVPK","execution":{"iopub.status.busy":"2024-02-26T00:02:10.692985Z","iopub.execute_input":"2024-02-26T00:02:10.693369Z","iopub.status.idle":"2024-02-26T00:02:10.703607Z","shell.execute_reply.started":"2024-02-26T00:02:10.693339Z","shell.execute_reply":"2024-02-26T00:02:10.702593Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Fine-Tuned Model\nacc = valid(model, testing_loader)\nprint(\"Accuracy on test data = %0.2f%%\" % acc)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZFmMUhiiOZyp","outputId":"0abc4785-b900-4eca-dae0-e161ff29f9f3","execution":{"iopub.status.busy":"2024-02-26T00:02:13.447375Z","iopub.execute_input":"2024-02-26T00:02:13.448038Z","iopub.status.idle":"2024-02-26T00:04:38.514405Z","shell.execute_reply.started":"2024-02-26T00:02:13.448006Z","shell.execute_reply":"2024-02-26T00:04:38.513480Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"This is the validation section to print the accuracy and see how it performs\nHere we are leveraging on the dataloader crearted for the validation dataset, the approcah is using more of pytorch\nBatch Validation Loss: 0.5136920809745789\nBatch Validation Accuracy: 50.0\nBatch Validation Loss: 0.028118200543703453\nBatch Validation Accuracy: 98.48484848484848\nBatch Validation Loss: 0.08958293956776078\nBatch Validation Accuracy: 96.92307692307692\nBatch Validation Loss: 0.07939069077478166\nBatch Validation Accuracy: 97.42268041237114\nBatch Validation Loss: 0.09454923810603372\nBatch Validation Accuracy: 96.89922480620154\nBatch Validation Loss: 0.1001064675897707\nBatch Validation Accuracy: 96.8944099378882\nBatch Validation Loss: 0.10475588827804116\nBatch Validation Accuracy: 96.89119170984456\nBatch Validation Loss: 0.0948363207715253\nBatch Validation Accuracy: 97.11111111111111\nBatch Validation Loss: 0.08371389490472522\nBatch Validation Accuracy: 97.47081712062257\nBatch Validation Loss: 0.08502557996344659\nBatch Validation Accuracy: 97.40484429065744\nBatch Validation Loss: 0.079620346965158\nBatch Validation Accuracy: 97.66355140186916\nBatch Validation Loss: 0.07331417123299018\nBatch Validation Accuracy: 97.87535410764872\nBatch Validation Loss: 0.0739921529765253\nBatch Validation Accuracy: 97.92207792207792\nBatch Validation Loss: 0.07016113012743225\nBatch Validation Accuracy: 98.08153477218225\nBatch Validation Loss: 0.06616679350337333\nBatch Validation Accuracy: 98.21826280623608\nBatch Validation Loss: 0.0676633496526745\nBatch Validation Accuracy: 98.12889812889813\nBatch Validation Loss: 0.06817660130992474\nBatch Validation Accuracy: 98.05068226120858\nBatch Validation Loss: 0.07430890198164714\nBatch Validation Accuracy: 97.88990825688073\nBatch Validation Loss: 0.07053927783862377\nBatch Validation Accuracy: 98.00693240901214\nBatch Validation Loss: 0.07020805575097307\nBatch Validation Accuracy: 98.0295566502463\nBatch Validation Loss: 0.06974499239026918\nBatch Validation Accuracy: 98.04992199687987\nBatch Validation Loss: 0.06819327309434295\nBatch Validation Accuracy: 98.06835066864785\nBatch Validation Loss: 0.06771203548521966\nBatch Validation Accuracy: 98.08510638297872\nBatch Validation Loss: 0.06512125366300425\nBatch Validation Accuracy: 98.16824966078697\nBatch Validation Loss: 0.07023171456022036\nBatch Validation Accuracy: 98.04941482444734\nBatch Validation Loss: 0.06872051707195692\nBatch Validation Accuracy: 98.0649188514357\nBatch Validation Loss: 0.06900524299609669\nBatch Validation Accuracy: 98.07923169267707\nBatch Validation Loss: 0.06892913071212568\nBatch Validation Accuracy: 98.03468208092485\nBatch Validation Loss: 0.06665980372756487\nBatch Validation Accuracy: 98.10479375696767\nBatch Validation Loss: 0.0664710765609275\nBatch Validation Accuracy: 98.11625403659849\nBatch Validation Loss: 0.06915731281168085\nBatch Validation Accuracy: 98.07492195629553\nBatch Validation Loss: 0.06846297908506815\nBatch Validation Accuracy: 98.08660624370594\nBatch Validation Loss: 0.068156062838417\nBatch Validation Accuracy: 98.09756097560975\nBatch Validation Loss: 0.06866209225728227\nBatch Validation Accuracy: 98.06054872280038\nBatch Validation Loss: 0.06758334914233477\nBatch Validation Accuracy: 98.07162534435261\nBatch Validation Loss: 0.06781977808363004\nBatch Validation Accuracy: 98.03746654772524\nBatch Validation Loss: 0.06729786870867847\nBatch Validation Accuracy: 98.04856895056375\nBatch Validation Loss: 0.06704155803296542\nBatch Validation Accuracy: 98.05907172995781\nBatch Validation Loss: 0.06910057507161489\nBatch Validation Accuracy: 97.98685291700905\nBatch Validation Loss: 0.06850937459121247\nBatch Validation Accuracy: 97.99839871897518\nBatch Validation Loss: 0.06740060666173255\nBatch Validation Accuracy: 98.04839968774395\nBatch Validation Loss: 0.0673395171300462\nBatch Validation Accuracy: 98.05788271134806\nBatch Validation Loss: 0.06689127583340586\nBatch Validation Accuracy: 98.06691449814126\nBatch Validation Loss: 0.06700885065404473\nBatch Validation Accuracy: 98.07552650689905\nBatch Validation Loss: 0.06572217343975258\nBatch Validation Accuracy: 98.11923349893542\nBatch Validation Loss: 0.06656999419211286\nBatch Validation Accuracy: 98.09160305343511\nBatch Validation Loss: 0.06743363450007553\nBatch Validation Accuracy: 98.06517311608961\nBatch Validation Loss: 0.06743881256842485\nBatch Validation Accuracy: 98.07308970099668\nBatch Validation Loss: 0.06629723192388937\nBatch Validation Accuracy: 98.11320754716981\nBatch Validation Loss: 0.06505813763757076\nBatch Validation Accuracy: 98.1516889738687\nBatch Validation Loss: 0.0649798987586014\nBatch Validation Accuracy: 98.15740162398501\nBatch Validation Loss: 0.0645240119326773\nBatch Validation Accuracy: 98.16289038579302\nBatch Validation Loss: 0.06589625566154607\nBatch Validation Accuracy: 98.10810810810811\nBatch Validation Loss: 0.0674216151142819\nBatch Validation Accuracy: 98.08485562757808\nBatch Validation Loss: 0.06627549240896193\nBatch Validation Accuracy: 98.1203007518797\nBatch Validation Loss: 0.06535078066931102\nBatch Validation Accuracy: 98.15445769449177\nBatch Validation Loss: 0.06429642777953856\nBatch Validation Accuracy: 98.18739542665924\nBatch Validation Loss: 0.06430993962101639\nBatch Validation Accuracy: 98.1917808219178\nBatch Validation Loss: 0.06329506889773522\nBatch Validation Accuracy: 98.22294022617125\nBatch Validation Loss: 0.062311510962807334\nBatch Validation Accuracy: 98.25304393859184\nBatch Validation Loss: 0.06258073720956422\nBatch Validation Accuracy: 98.25611660593441\nBatch Validation Loss: 0.06279910663314472\nBatch Validation Accuracy: 98.25908858166923\nBatch Validation Loss: 0.06547603282798316\nBatch Validation Accuracy: 98.18639798488665\nBatch Validation Loss: 0.06457259475454839\nBatch Validation Accuracy: 98.21517104610808\nBatch Validation Loss: 0.06387386664195034\nBatch Validation Accuracy: 98.24304538799414\nBatch Validation Loss: 0.0642021677141507\nBatch Validation Accuracy: 98.22200864968765\nBatch Validation Loss: 0.0676378317754028\nBatch Validation Accuracy: 98.13061997160436\nBatch Validation Loss: 0.06671478183872755\nBatch Validation Accuracy: 98.15850815850816\nBatch Validation Loss: 0.06632625099242018\nBatch Validation Accuracy: 98.16260909508497\nBatch Validation Loss: 0.06683121835888815\nBatch Validation Accuracy: 98.14395654142146\nBatch Validation Loss: 0.0660366333374028\nBatch Validation Accuracy: 98.17045961624275\nBatch Validation Loss: 0.06651637498662204\nBatch Validation Accuracy: 98.152221733392\nBatch Validation Loss: 0.06567907924329511\nBatch Validation Accuracy: 98.17787418655098\nBatch Validation Loss: 0.06492746825607722\nBatch Validation Accuracy: 98.20282413350449\nBatch Validation Loss: 0.06415290908965429\nBatch Validation Accuracy: 98.2271000422119\nBatch Validation Loss: 0.06349948089116364\nBatch Validation Accuracy: 98.25072886297376\nBatch Validation Loss: 0.0630539205279186\nBatch Validation Accuracy: 98.2531853678586\nBatch Validation Loss: 0.06266552882720518\nBatch Validation Accuracy: 98.27586206896552\nBatch Validation Loss: 0.06252368351293174\nBatch Validation Accuracy: 98.27793352022427\nBatch Validation Loss: 0.06218100282757261\nBatch Validation Accuracy: 98.27995255041519\nBatch Validation Loss: 0.06153699203857287\nBatch Validation Accuracy: 98.30144474814526\nBatch Validation Loss: 0.06138898745197853\nBatch Validation Accuracy: 98.30312379483225\nBatch Validation Loss: 0.06078295735713272\nBatch Validation Accuracy: 98.32380952380953\nBatch Validation Loss: 0.06021196574027383\nBatch Validation Accuracy: 98.34399698908544\nBatch Validation Loss: 0.06013214287450956\nBatch Validation Accuracy: 98.32651543324656\nBatch Validation Loss: 0.059490883698428886\nBatch Validation Accuracy: 98.34619625137817\nBatch Validation Loss: 0.05886080983164898\nBatch Validation Accuracy: 98.36541954231747\nBatch Validation Loss: 0.058253240093247734\nBatch Validation Accuracy: 98.38420107719928\nBatch Validation Loss: 0.05795154342494632\nBatch Validation Accuracy: 98.38480653177139\nBatch Validation Loss: 0.05800021610969057\nBatch Validation Accuracy: 98.36784836784837\nBatch Validation Loss: 0.05866822861707183\nBatch Validation Accuracy: 98.35126692120791\nBatch Validation Loss: 0.058300786444337366\nBatch Validation Accuracy: 98.35221421215242\nBatch Validation Loss: 0.058889007574605086\nBatch Validation Accuracy: 98.33616298811545\nBatch Validation Loss: 0.058548669238097095\nBatch Validation Accuracy: 98.33725226738328\nBatch Validation Loss: 0.05870752868904299\nBatch Validation Accuracy: 98.32170156198073\nBatch Validation Loss: 0.0582540912774325\nBatch Validation Accuracy: 98.3393620519566\nBatch Validation Loss: 0.05781342541487397\nBatch Validation Accuracy: 98.35665473478686\nBatch Validation Loss: 0.05757745215789251\nBatch Validation Accuracy: 98.35748792270532\nBatch Validation Loss: 0.057555030253090875\nBatch Validation Accuracy: 98.34236531718201\nBatch Validation Loss: 0.05788054812285766\nBatch Validation Accuracy: 98.3275481224361\nBatch Validation Loss: 0.057445667049521976\nBatch Validation Accuracy: 98.34426741643236\nBatch Validation Loss: 0.056989810806305885\nBatch Validation Accuracy: 98.36065573770492\nBatch Validation Loss: 0.05667472514573479\nBatch Validation Accuracy: 98.3614088820827\nBatch Validation Loss: 0.056643356147869375\nBatch Validation Accuracy: 98.34698210494389\nBatch Validation Loss: 0.05754313163683669\nBatch Validation Accuracy: 98.33283268248724\nBatch Validation Loss: 0.058580165390314284\nBatch Validation Accuracy: 98.3040761678072\nBatch Validation Loss: 0.05878807444439972\nBatch Validation Accuracy: 98.2905982905983\nBatch Validation Loss: 0.06047142881879678\nBatch Validation Accuracy: 98.23357664233576\nBatch Validation Loss: 0.06087441368914585\nBatch Validation Accuracy: 98.22100086780445\nBatch Validation Loss: 0.060660039271914296\nBatch Validation Accuracy: 98.23731728288908\nBatch Validation Loss: 0.060267457140670985\nBatch Validation Accuracy: 98.25333712013632\nBatch Validation Loss: 0.05978468688028738\nBatch Validation Accuracy: 98.2690683929074\nBatch Validation Loss: 0.059675758537892226\nBatch Validation Accuracy: 98.25662482566248\nBatch Validation Loss: 0.059230020146416325\nBatch Validation Accuracy: 98.27204865910976\nBatch Validation Loss: 0.059139561495356\nBatch Validation Accuracy: 98.27349958892847\nBatch Validation Loss: 0.06023594559302123\nBatch Validation Accuracy: 98.23417549578919\nBatch Validation Loss: 0.06038813294735987\nBatch Validation Accuracy: 98.22246162133047\nBatch Validation Loss: 0.0609710476829985\nBatch Validation Accuracy: 98.2109479305741\nValidation Loss Epoch: 0.06090217182425161\nValidation Accuracy Epoch: 98.21333333333334\nAccuracy on test data = 98.21%\n","output_type":"stream"}]},{"cell_type":"code","source":"# Saving the files for re-use\n\n# output_model_file = '/'\n# output_vocab_file = '/'\n\nmodel_to_save = model\ntorch.save(model.state_dict(), '/kaggle/working/best_synthetic.pkl')\n# tokenizer.save_vocabulary(output_vocab_file)\n\nprint('Model saved successfully.')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pnbDkLT_ObPJ","outputId":"bcc1769a-61c2-4e77-e294-e40a101989fc","execution":{"iopub.status.busy":"2024-02-26T00:05:58.153889Z","iopub.execute_input":"2024-02-26T00:05:58.154642Z","iopub.status.idle":"2024-02-26T00:05:58.571952Z","shell.execute_reply.started":"2024-02-26T00:05:58.154608Z","shell.execute_reply":"2024-02-26T00:05:58.570893Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Model saved successfully.\n","output_type":"stream"}]},{"cell_type":"code","source":"base_model = DistillBERTClass()\nbase_model.to(device)\n# base_model = torch.nn.DataParallel(model, device_ids = [0,1]).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T00:17:52.200710Z","iopub.execute_input":"2024-02-26T00:17:52.201085Z","iopub.status.idle":"2024-02-26T00:17:52.639612Z","shell.execute_reply.started":"2024-02-26T00:17:52.201055Z","shell.execute_reply":"2024-02-26T00:17:52.638689Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"DistillBERTClass(\n  (l1): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0-5): 6 x TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (dropout): Dropout(p=0.3, inplace=False)\n  (classifier): Linear(in_features=768, out_features=7, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Base Model\nacc = valid(base_model, testing_loader)\nprint(\"Accuracy on test data = %0.2f%%\" % acc)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T00:17:54.431524Z","iopub.execute_input":"2024-02-26T00:17:54.432176Z","iopub.status.idle":"2024-02-26T00:19:57.627463Z","shell.execute_reply.started":"2024-02-26T00:17:54.432142Z","shell.execute_reply":"2024-02-26T00:19:57.626477Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Batch Validation Loss: 1.9485548734664917\nBatch Validation Accuracy: 50.0\nBatch Validation Loss: 1.9528504104325266\nBatch Validation Accuracy: 12.121212121212121\nBatch Validation Loss: 1.946119477198674\nBatch Validation Accuracy: 16.923076923076923\nBatch Validation Loss: 1.944229782242136\nBatch Validation Accuracy: 15.97938144329897\nBatch Validation Loss: 1.9435420378234036\nBatch Validation Accuracy: 15.891472868217054\nBatch Validation Loss: 1.9437079822054562\nBatch Validation Accuracy: 17.080745341614907\nBatch Validation Loss: 1.9465815551540395\nBatch Validation Accuracy: 16.83937823834197\nBatch Validation Loss: 1.9465919219122993\nBatch Validation Accuracy: 16.444444444444443\nBatch Validation Loss: 1.9492624274487624\nBatch Validation Accuracy: 15.56420233463035\nBatch Validation Loss: 1.947484680113083\nBatch Validation Accuracy: 15.22491349480969\nBatch Validation Loss: 1.9466168795047891\nBatch Validation Accuracy: 15.88785046728972\nBatch Validation Loss: 1.9473867004383725\nBatch Validation Accuracy: 15.722379603399434\nBatch Validation Loss: 1.9476036632215823\nBatch Validation Accuracy: 15.714285714285714\nBatch Validation Loss: 1.9486565241138998\nBatch Validation Accuracy: 15.467625899280575\nBatch Validation Loss: 1.9504729923532906\nBatch Validation Accuracy: 15.478841870824054\nBatch Validation Loss: 1.9499821945436284\nBatch Validation Accuracy: 15.696465696465696\nBatch Validation Loss: 1.950924195044222\nBatch Validation Accuracy: 15.789473684210526\nBatch Validation Loss: 1.9520673460916642\nBatch Validation Accuracy: 15.229357798165138\nBatch Validation Loss: 1.9503672195265984\nBatch Validation Accuracy: 15.684575389948007\nBatch Validation Loss: 1.9500527943687878\nBatch Validation Accuracy: 15.763546798029557\nBatch Validation Loss: 1.9498584220264334\nBatch Validation Accuracy: 15.990639625585024\nBatch Validation Loss: 1.9499245628736703\nBatch Validation Accuracy: 15.898959881129272\nBatch Validation Loss: 1.9486305377162094\nBatch Validation Accuracy: 16.02836879432624\nBatch Validation Loss: 1.9486328790436929\nBatch Validation Accuracy: 16.078697421981005\nBatch Validation Loss: 1.9478027191211715\nBatch Validation Accuracy: 16.44993498049415\nBatch Validation Loss: 1.9478415963652727\nBatch Validation Accuracy: 16.16729088639201\nBatch Validation Loss: 1.9482236153223649\nBatch Validation Accuracy: 16.02641056422569\nBatch Validation Loss: 1.948299570441935\nBatch Validation Accuracy: 15.895953757225433\nBatch Validation Loss: 1.9484685111869804\nBatch Validation Accuracy: 15.830546265328874\nBatch Validation Loss: 1.9480952854690305\nBatch Validation Accuracy: 16.03875134553283\nBatch Validation Loss: 1.9475964244521\nBatch Validation Accuracy: 16.024973985431842\nBatch Validation Loss: 1.9470966582571991\nBatch Validation Accuracy: 16.2134944612286\nBatch Validation Loss: 1.9470572341360697\nBatch Validation Accuracy: 16.146341463414632\nBatch Validation Loss: 1.947011646732702\nBatch Validation Accuracy: 16.225165562913908\nBatch Validation Loss: 1.946627426519648\nBatch Validation Accuracy: 16.391184573002754\nBatch Validation Loss: 1.9474664956304668\nBatch Validation Accuracy: 16.190900981266726\nBatch Validation Loss: 1.946968503031437\nBatch Validation Accuracy: 16.175195143104943\nBatch Validation Loss: 1.947770208044897\nBatch Validation Accuracy: 16.075949367088608\nBatch Validation Loss: 1.9475998101708532\nBatch Validation Accuracy: 16.1873459326212\nBatch Validation Loss: 1.9479807169557668\nBatch Validation Accuracy: 15.972778222578063\nBatch Validation Loss: 1.9485834231142138\nBatch Validation Accuracy: 15.729898516783763\nBatch Validation Loss: 1.9490527321342648\nBatch Validation Accuracy: 15.575019040365575\nBatch Validation Loss: 1.9487488753733582\nBatch Validation Accuracy: 15.650557620817844\nBatch Validation Loss: 1.9481872913191607\nBatch Validation Accuracy: 15.831517792302106\nBatch Validation Loss: 1.9485265571568517\nBatch Validation Accuracy: 15.755855216465578\nBatch Validation Loss: 1.9488156032926254\nBatch Validation Accuracy: 15.7529493407356\nBatch Validation Loss: 1.9489124337589345\nBatch Validation Accuracy: 15.784114052953157\nBatch Validation Loss: 1.9493910519387634\nBatch Validation Accuracy: 15.714285714285714\nBatch Validation Loss: 1.9496485900785834\nBatch Validation Accuracy: 15.582303188028627\nBatch Validation Loss: 1.9496916428426787\nBatch Validation Accuracy: 15.61504142766093\nBatch Validation Loss: 1.94963587432411\nBatch Validation Accuracy: 15.646470955652717\nBatch Validation Loss: 1.9491990705392\nBatch Validation Accuracy: 15.737905695039805\nBatch Validation Loss: 1.9492041635083723\nBatch Validation Accuracy: 15.705705705705705\nBatch Validation Loss: 1.949791278990845\nBatch Validation Accuracy: 15.556865055981143\nBatch Validation Loss: 1.9498941645310612\nBatch Validation Accuracy: 15.558126084441874\nBatch Validation Loss: 1.9504481709050834\nBatch Validation Accuracy: 15.445769449176604\nBatch Validation Loss: 1.9502408781732452\nBatch Validation Accuracy: 15.39319576129392\nBatch Validation Loss: 1.9502321579031747\nBatch Validation Accuracy: 15.452054794520548\nBatch Validation Loss: 1.950576324198409\nBatch Validation Accuracy: 15.293484114162627\nBatch Validation Loss: 1.9506663397041308\nBatch Validation Accuracy: 15.193223928004235\nBatch Validation Loss: 1.950791851609154\nBatch Validation Accuracy: 15.148360229047372\nBatch Validation Loss: 1.951078459414469\nBatch Validation Accuracy: 15.053763440860216\nBatch Validation Loss: 1.950899641699995\nBatch Validation Accuracy: 15.062972292191436\nBatch Validation Loss: 1.9509825330000439\nBatch Validation Accuracy: 15.047099652949926\nBatch Validation Loss: 1.9504558678776418\nBatch Validation Accuracy: 15.178135675939483\nBatch Validation Loss: 1.9500116532035656\nBatch Validation Accuracy: 15.209034118212397\nBatch Validation Loss: 1.9502278325459543\nBatch Validation Accuracy: 15.120681495504023\nBatch Validation Loss: 1.9500877826086014\nBatch Validation Accuracy: 15.151515151515152\nBatch Validation Loss: 1.9502797072011504\nBatch Validation Accuracy: 15.181442351860358\nBatch Validation Loss: 1.9501111514431257\nBatch Validation Accuracy: 15.233137166138524\nBatch Validation Loss: 1.949688706532061\nBatch Validation Accuracy: 15.305667112896028\nBatch Validation Loss: 1.9496480028271623\nBatch Validation Accuracy: 15.332160140783106\nBatch Validation Loss: 1.950131817391534\nBatch Validation Accuracy: 15.271149674620391\nBatch Validation Loss: 1.9499481949583812\nBatch Validation Accuracy: 15.254599914420197\nBatch Validation Loss: 1.950347284215973\nBatch Validation Accuracy: 15.175179400590967\nBatch Validation Loss: 1.9503893416804703\nBatch Validation Accuracy: 15.181174510620576\nBatch Validation Loss: 1.9503405266615572\nBatch Validation Accuracy: 15.125359638306618\nBatch Validation Loss: 1.9501837581940162\nBatch Validation Accuracy: 15.172413793103448\nBatch Validation Loss: 1.9500148833919917\nBatch Validation Accuracy: 15.198237885462555\nBatch Validation Loss: 1.9499601310965289\nBatch Validation Accuracy: 15.20363780150257\nBatch Validation Loss: 1.950104288364099\nBatch Validation Accuracy: 15.208902772354548\nBatch Validation Loss: 1.9500001504344917\nBatch Validation Accuracy: 15.175472425761667\nBatch Validation Loss: 1.9499380479994275\nBatch Validation Accuracy: 15.161904761904761\nBatch Validation Loss: 1.9499906622739855\nBatch Validation Accuracy: 15.18630033872789\nBatch Validation Loss: 1.9499902227369106\nBatch Validation Accuracy: 15.210115284492376\nBatch Validation Loss: 1.950101695793247\nBatch Validation Accuracy: 15.251745681734656\nBatch Validation Loss: 1.9500374972408916\nBatch Validation Accuracy: 15.29240828187432\nBatch Validation Loss: 1.9501174612576058\nBatch Validation Accuracy: 15.314183123877918\nBatch Validation Loss: 1.9498450469581379\nBatch Validation Accuracy: 15.406460773872915\nBatch Validation Loss: 1.9493149995971204\nBatch Validation Accuracy: 15.566865566865566\nBatch Validation Loss: 1.9492137469338362\nBatch Validation Accuracy: 15.550156195765359\nBatch Validation Loss: 1.9491780380837662\nBatch Validation Accuracy: 15.516649502231376\nBatch Validation Loss: 1.9494730745390032\nBatch Validation Accuracy: 15.432937181663837\nBatch Validation Loss: 1.9491123498026701\nBatch Validation Accuracy: 15.535774269398724\nBatch Validation Loss: 1.9493179729746282\nBatch Validation Accuracy: 15.536723163841808\nBatch Validation Loss: 1.9488809755937475\nBatch Validation Accuracy: 15.636303847418612\nBatch Validation Loss: 1.9487130217159605\nBatch Validation Accuracy: 15.71753986332574\nBatch Validation Loss: 1.94882444003162\nBatch Validation Accuracy: 15.68438003220612\nBatch Validation Loss: 1.9491601681154473\nBatch Validation Accuracy: 15.651896716608224\nBatch Validation Loss: 1.9490033537478582\nBatch Validation Accuracy: 15.65162511833386\nBatch Validation Loss: 1.9488764455712464\nBatch Validation Accuracy: 15.729459543892533\nBatch Validation Loss: 1.9491906165707933\nBatch Validation Accuracy: 15.712960098979277\nBatch Validation Loss: 1.948952665628372\nBatch Validation Accuracy: 15.788667687595712\nBatch Validation Loss: 1.9486024160619428\nBatch Validation Accuracy: 15.847740370033364\nBatch Validation Loss: 1.9486742827870103\nBatch Validation Accuracy: 15.845599279062782\nBatch Validation Loss: 1.948620367532541\nBatch Validation Accuracy: 15.858375483487057\nBatch Validation Loss: 1.9485395378396113\nBatch Validation Accuracy: 15.870910698496905\nBatch Validation Loss: 1.9483129255615013\nBatch Validation Accuracy: 15.897810218978103\nBatch Validation Loss: 1.9485046734587803\nBatch Validation Accuracy: 15.837431298814\nBatch Validation Loss: 1.9485678888385431\nBatch Validation Accuracy: 15.82115219260533\nBatch Validation Loss: 1.9484994761207775\nBatch Validation Accuracy: 15.847770519738711\nBatch Validation Loss: 1.9488970483822317\nBatch Validation Accuracy: 15.803546298902337\nBatch Validation Loss: 1.9487977527674272\nBatch Validation Accuracy: 15.815899581589958\nBatch Validation Loss: 1.9487252667822297\nBatch Validation Accuracy: 15.80038706110036\nBatch Validation Loss: 1.9490505198839103\nBatch Validation Accuracy: 15.744039462866539\nBatch Validation Loss: 1.9491338759004924\nBatch Validation Accuracy: 15.756587883727248\nBatch Validation Loss: 1.949040186427656\nBatch Validation Accuracy: 15.768920010772959\nBatch Validation Loss: 1.9490014240483895\nBatch Validation Accuracy: 15.807743658210947\nValidation Loss Epoch: 1.9490233685175578\nValidation Accuracy Epoch: 15.8\nAccuracy on test data = 15.80%\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}